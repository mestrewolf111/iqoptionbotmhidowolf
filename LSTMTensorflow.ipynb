{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1EWEqVotHPkCUNHpWMgW9CGwiBHQZ15oZ",
      "authorship_tag": "ABX9TyOew2W/wFZlckHdFKbjTHIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mestrewolf111/iqoptionbotmhidowolf/blob/main/LSTMTensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwAQkyF9oD3L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import deque\n",
        "import seaborn as sns\n",
        "from requests import post\n",
        "import sys\n",
        "from collections import deque\n",
        "from iqoptionapi.stable_api import IQ_Option\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Spyder Editor\n",
        "\n",
        "Este é um arquivo de script temporário.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import time\n",
        "from iqoptionapi.stable_api import IQ_Option\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "while True:\n",
        "    try:\n",
        "        email = (\n",
        "            input(\"Digite seu email:: \")\n",
        "        )\n",
        "        break\n",
        "    except:\n",
        "        print(\"\\n Errou\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        password = (\n",
        "            input(\"Digite sua senha:: \")\n",
        "        )\n",
        "        break\n",
        "    except:\n",
        "        print(\"\\n errou\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        pariedade = (\n",
        "            input(\"Digite a moeda que deseja operar:: \")\n",
        "        )\n",
        "        break\n",
        "    except:\n",
        "        print(\"\\n errou\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        stoploss = (\n",
        "            input(\"STOPLOSS:: \")\n",
        "        )\n",
        "        break\n",
        "    except:\n",
        "        print(\"\\n errou\")\n",
        "\n",
        "def login(verbose=False, iq=None, checkConnection=False):\n",
        "    if verbose:\n",
        "        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')\n",
        "\n",
        "    if iq == None:\n",
        "        print(\"Trying to connect to IqOption\")\n",
        "        iq = IQ_Option(f\"{email}\", f\"{password}\")\n",
        "        iq.connect()  # connect to iqoption\n",
        "        iq.get_all_init()\n",
        "    if iq != None:\n",
        "        while True:\n",
        "            if iq.check_connect() == False:\n",
        "                print('Error when trying to connect')\n",
        "                print(iq)\n",
        "                print(\"Retrying\")\n",
        "                iq.connect()\n",
        "            else:\n",
        "                if not checkConnection:\n",
        "                    print('Successfully Connected!')\n",
        "                break\n",
        "            time.sleep(3)\n",
        "\n",
        "    iq.change_balance(\"PRACTICE\")  # or real\n",
        "    return iq\n",
        "\n",
        "\n",
        "def higher(iq, Money, Actives):\n",
        "    check, id = iq.buy(Money, Actives, \"call\", 1)\n",
        "\n",
        "    if not check:\n",
        "        print('Error call')\n",
        "        print(check, id)\n",
        "        exit(0)\n",
        "\n",
        "    return id\n",
        "\n",
        "\n",
        "def lower(iq, Money, Actives):\n",
        "    check, id = iq.buy(Money, Actives, \"put\", 1)\n",
        "    if not check:\n",
        "        print('Error put')\n",
        "        print(check, id)\n",
        "        exit(0)\n",
        "\n",
        "    return id\n",
        "\n",
        "\n",
        "def get_candles(iq, Actives):\n",
        "    login(iq=iq, checkConnection=True)\n",
        "    return iq.get_candles(Actives, 60, 1000, time.time())\n",
        "\n",
        "\n",
        "def get_all_candles(iq, Actives, start_candle):\n",
        "    # demora um minuto\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    for x in range(1):\n",
        "        login(iq=iq, checkConnection=True)\n",
        "        data = iq.get_candles(Actives, 60, 1000, start_candle)\n",
        "        start_candle = data[0]['to'] - 1\n",
        "        final_data.extend(data)\n",
        "    return final_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_data_needed(iq): #function to gather all the data\n",
        "    start_candle = time.time()\n",
        "    actives = [f'{pariedade}']\n",
        "    final_data = pd.DataFrame()\n",
        "    for active in actives:\n",
        "        current = get_all_candles(iq,active,start_candle)\n",
        "        main = pd.DataFrame()\n",
        "        useful_frame = pd.DataFrame()\n",
        "        for candle in current:\n",
        "            useful_frame = pd.DataFrame(list(candle.values()),index = list(candle.keys())).T.drop(columns = ['at'])\n",
        "            useful_frame = useful_frame.set_index(useful_frame['id']).drop(columns = ['id'])\n",
        "            main = main.append(useful_frame)\n",
        "            main.drop_duplicates()\n",
        "        if active == f'{pariedade}':\n",
        "            final_data = main.drop(columns = {'from','to'})\n",
        "        else:\n",
        "            main = main.drop(columns = {'from','to','open','min','max'})\n",
        "            main.columns = [f'close_{active}',f'volume_{active}']\n",
        "            final_data = final_data.join(main)\n",
        "    final_data = final_data.loc[~final_data.index.duplicated(keep = 'first')]\n",
        "    #print(final_data)\n",
        "    return final_data\n",
        "\n",
        "def get_data_neededOTC(iq): #function to gather all the data\n",
        "    start_candle = time.time()\n",
        "    actives = ['EURUSD-OTC','NZDUSD-OTC','EURGBP-OTC','EURJPY-OTC','GBPUSD-OTC','USDCHF-OTC']\n",
        "    final_data = pd.DataFrame()\n",
        "    for active in actives:\n",
        "        current = get_all_candles(iq,active,start_candle)\n",
        "        main = pd.DataFrame()\n",
        "        for candle in current:\n",
        "            useful_frame = pd.DataFrame(list(candle.values()),index = list(candle.keys())).T.drop(columns = ['at'])\n",
        "            useful_frame = useful_frame.set_index(useful_frame['id']).drop(columns = ['id'])\n",
        "            main = main.append(useful_frame)\n",
        "            main.drop_duplicates()\n",
        "        if active == 'EURUSD-OTC':\n",
        "            final_data = main.drop(columns = {'from','to'})\n",
        "        else:\n",
        "            main = main.drop(columns = {'from','to','open','min','max'})\n",
        "            main.columns = [f'close_{active}',f'volume_{active}']\n",
        "            final_data = final_data.join(main)\n",
        "    final_data = final_data.loc[~final_data.index.duplicated(keep = 'first')]\n",
        "    return final_data\n",
        "\n",
        "def get_1candles(iq,Actives):\n",
        "    login(iq = iq, checkConnection = True)\n",
        "    df = pd.DataFrame(iq.get_candles(Actives, 60, 2, time.time()))\n",
        "    df.columns = ['id','from','at','to','Open','Close','Low','High','Volume']\n",
        "    return df\n",
        "\n",
        "\n",
        "def fast_data(iq, ratio):  # function to gather reduced data for the testing\n",
        "    login(iq=iq, checkConnection=True)\n",
        "    candles = iq.get_candles(ratio, 60, 1000, time.time())\n",
        "    useful_frame = pd.DataFrame()\n",
        "    main = pd.DataFrame()\n",
        "    for candle in candles:\n",
        "        useful_frame = pd.DataFrame(list(candle.values()), index=list(candle.keys())).T.drop(columns=['at'])\n",
        "        useful_frame = useful_frame.set_index(useful_frame['id']).drop(columns=['id'])\n",
        "        main = main.append(useful_frame)\n",
        "    return main\n",
        "\n",
        "\n",
        "def get_balance(iq):\n",
        "    return iq.get_balance()\n",
        "\n",
        "\n",
        "def get_profit(iq):\n",
        "    return iq.get_all_profit()[f'{pariedade}']['turbo']\n",
        "\n",
        "SEQ_LEN = 5  # how long\n",
        "FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict\n",
        "\n",
        "\n",
        "def classify(current, future):\n",
        "    if float(future) > float(current):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    indexes = df.index\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    df = pd.DataFrame(df_scaled, index=indexes)\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(\n",
        "        maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if  put\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # if call\n",
        "            buys.append([seq, target])\n",
        "\n",
        "    random.shuffle(buys)\n",
        "    random.shuffle(sells)  # shuffle\n",
        "\n",
        "    lower = min(len(buys), len(sells))\n",
        "\n",
        "    buys = buys[:lower]\n",
        "    sells = sells[:lower]\n",
        "\n",
        "    sequential_data = buys + sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets\n",
        "\n",
        "    return np.array(X), y\n",
        "\n",
        "def train_data():\n",
        "    iq = login()\n",
        "\n",
        "    actives = [f'{pariedade}']\n",
        "\n",
        "    df = get_data_needed(iq)\n",
        "\n",
        "    df.isnull().sum().sum()  # there are no nans\n",
        "    df.fillna(method=\"ffill\", inplace=True)\n",
        "    df = df.loc[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    df['future'] = df[\"close\"].shift(-FUTURE_PERIOD_PREDICT)  # future prediction\n",
        "\n",
        "    df['MA_9'] = df['close'].rolling(window=9).mean()  # moving average 20\n",
        "    df['MA_21'] = df['close'].rolling(window=21).mean()  # moving average 50\n",
        "\n",
        "    df['L14'] = df['min'].rolling(window=14).min()\n",
        "    df['H14'] = df['max'].rolling(window=14).max()\n",
        "    df['%K'] = 100 * ((df['close'] - df['L14']) / (df['H14'] - df['L14']))  # stochastic oscilator\n",
        "    df['%D'] = df['%K'].rolling(window=3).mean()\n",
        "\n",
        "    df['EMA_30'] = df['close'].ewm(span=30, adjust=False).mean()  # exponential moving average\n",
        "    df['EMA_50'] = df['close'].ewm(span=50, adjust=False).mean()\n",
        "\n",
        "\n",
        "    rsi_period = 8\n",
        "    chg = df['close'].diff(1)\n",
        "    gain = chg.mask(chg < 0, 0)\n",
        "    df['gain'] = gain\n",
        "    loss = chg.mask(chg > 0, 0)\n",
        "    df['loss'] = loss\n",
        "    avg_gain = gain.ewm(com=rsi_period - 1, min_periods=rsi_period).mean()\n",
        "    avg_loss = loss.ewm(com=rsi_period - 1, min_periods=rsi_period).mean()\n",
        "\n",
        "    df['avg_gain'] = avg_gain\n",
        "    df['avg_loss'] = avg_loss\n",
        "    rs = abs(avg_gain / avg_loss)\n",
        "    df['rsi'] = 100 - (100 / (1 + rs))  # rsi index\n",
        "\n",
        "    df = df.drop(columns={'open', 'min', 'max', 'avg_gain', 'avg_loss', 'L14', 'H14', 'gain',\n",
        "                          'loss'})  # drop columns that are too correlated or are in somehow inside others\n",
        "\n",
        "    df = df.dropna()\n",
        "    dataset = df.fillna(method=\"ffill\")\n",
        "    dataset = dataset.dropna()\n",
        "\n",
        "    dataset.sort_index(inplace=True)\n",
        "\n",
        "    main_df = dataset\n",
        "\n",
        "    main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "    main_df.dropna(inplace=True)\n",
        "\n",
        "    main_df['target'] = list(map(classify, main_df['close'], main_df['future']))\n",
        "\n",
        "    main_df.dropna(inplace=True)\n",
        "\n",
        "    main_df['target'].value_counts()\n",
        "\n",
        "    main_df.dropna(inplace=True)\n",
        "\n",
        "    main_df = main_df.astype('float32')\n",
        "\n",
        "    times = sorted(main_df.index.values)\n",
        "    last_5pct = sorted(main_df.index.values)[-int(0.1 * len(times))]\n",
        "\n",
        "    validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
        "    main_df = main_df[(main_df.index < last_5pct)]\n",
        "\n",
        "    train_x, train_y = preprocess_df(main_df)\n",
        "    validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "    print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "    print(f\"sells: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "    print(f\"VALIDATION sells: {validation_y.count(0)}, buys : {validation_y.count(1)}\")\n",
        "\n",
        "    train_y = np.asarray(train_y)\n",
        "    validation_y = np.asarray(validation_y)\n",
        "\n",
        "    LEARNING_RATE = 0.007  # isso mesmo\n",
        "    EPOCHS = 14# how many passes through our data #20 was good\n",
        "    BATCH_SIZE = 32  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
        "    NAME = f\"{LEARNING_RATE}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-{EPOCHS}-{BATCH_SIZE}-PRED-{int(time.time())}\"  # a unique name for the model\n",
        "    print(NAME)\n",
        "    try:\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            # Currently, memory growth needs to be the same across GPUs\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except Exception as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "    earlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', mode='max', verbose=5, patience=30)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())  # normalizes activation outputs, same reason you want to normalize your input data.\n",
        "\n",
        "    model.add(LSTM(128, return_sequences=True))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(lr=LEARNING_RATE, decay=5e-5)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
        "\n",
        "    filepath = \"LSTM-best\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
        "    checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath), monitor='accuracy', verbose=1, save_best_only=True,\n",
        "                                 mode='max', save_format=\"h5\")  # saves only the best ones\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_x, train_y,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=(validation_x, validation_y),\n",
        "        callbacks=[tensorboard, checkpoint, earlyStoppingCallback],\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess_prediciton(iq):\n",
        "    Actives = [f'{pariedade}']\n",
        "    active = f'{pariedade}'\n",
        "    main = pd.DataFrame()\n",
        "    current = pd.DataFrame()\n",
        "    for active in Actives:\n",
        "        if active == f'{pariedade}':\n",
        "            main = fast_data(iq, active).drop(columns={'from', 'to'})\n",
        "        else:\n",
        "            current = fast_data(iq, active)\n",
        "            current = current.drop(columns={'from', 'to', 'open', 'min', 'max'})\n",
        "            current.columns = [f'close_{active}', f'volume_{active}']\n",
        "            main = main.join(current)\n",
        "\n",
        "\n",
        "    df = main\n",
        "\n",
        "    \"\"\"\n",
        "    graphical analysis components\n",
        "    \"\"\"\n",
        "\n",
        "    df.isnull().sum().sum()  # there are no nans\n",
        "    df.fillna(method=\"ffill\", inplace=True)\n",
        "    df = df.loc[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    df['MA_9'] = df['close'].rolling(window=3).mean()\n",
        "    df['MA_21'] = df['close'].rolling(window=9).mean()\n",
        "\n",
        "    df['L14'] = df['min'].rolling(window=14).min()\n",
        "    df['H14'] = df['max'].rolling(window=14).max()\n",
        "    df['%K'] = 100 * ((df['close'] - df['L14']) / (df['H14'] - df['L14']))\n",
        "    df['%D'] = df['%K'].rolling(window=3).mean()\n",
        "\n",
        "    df['EMA_30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
        "    df['EMA_50'] = df['close'].ewm(span=50, adjust=False).mean()\n",
        "\n",
        "\n",
        "    rsi_period = 14\n",
        "    chg = df['close'].diff(1)\n",
        "    gain = chg.mask(chg < 0, 0)\n",
        "    df['gain'] = gain\n",
        "    loss = chg.mask(chg > 0, 0)\n",
        "    df['loss'] = loss\n",
        "    avg_gain = gain.ewm(com=rsi_period - 1, min_periods=rsi_period).mean()\n",
        "    avg_loss = loss.ewm(com=rsi_period - 1, min_periods=rsi_period).mean()\n",
        "\n",
        "    df['avg_gain'] = avg_gain\n",
        "    df['avg_loss'] = avg_loss\n",
        "    rs = abs(avg_gain / avg_loss)\n",
        "    df['rsi'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    \"\"\"\n",
        "    Finishing preprocessing\n",
        "    \"\"\"\n",
        "    df = df.drop(columns={'open', 'min', 'max', 'avg_gain', 'avg_loss', 'L14', 'H14', 'gain', 'loss'})\n",
        "\n",
        "    df = df.dropna()\n",
        "    df = df.fillna(method=\"ffill\")\n",
        "    df = df.dropna()\n",
        "\n",
        "    df.sort_index(inplace=True)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    indexes = df.index\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    pred = pd.DataFrame(df_scaled, index=indexes)\n",
        "\n",
        "    sequential_data = []\n",
        "    prev_days = deque(maxlen=SEQ_LEN)\n",
        "\n",
        "    for i in pred.iloc[len(pred) - SEQ_LEN:len(pred), :].values:\n",
        "        prev_days.append([n for n in i[:]])\n",
        "        if len(prev_days) == SEQ_LEN:\n",
        "            sequential_data.append([np.array(prev_days)])\n",
        "\n",
        "    X = []\n",
        "\n",
        "    for seq in sequential_data:\n",
        "        X.append(seq)\n",
        "\n",
        "    return np.array(X)\n",
        "\n",
        "\n",
        "\n",
        "if (len(sys.argv) == 1):\n",
        "    martingale = 2\n",
        "    bet_money = 100\n",
        "    ratio = f'{pariedade}'\n",
        "elif (len(sys.argv) != 3):\n",
        "    print(\n",
        "        \"The correct pattern is: python testing.py EURUSD (or other currency) INITIAL_BET(value starting in 1$ MIN) MARTINGALE (your martingale ratio default = 2)\")\n",
        "    print(\"\\n\\nEXAMPLE:\\npython testing.py EURUSD 1 3\")\n",
        "    exit(-1)\n",
        "else:\n",
        "    bet_money = sys.argv[2]  # QUANTITY YOU WANT TO BET EACH TIME\n",
        "    ratio = sys.argv[1]\n",
        "    martingale = sys.argv[2]\n",
        "\n",
        "SEQ_LEN = 5  # how long of a preceeding sequence to collect for RNN, if you modify here, remember to modify in the other files too\n",
        "FUTURE_PERIOD_PREDICT = 2  # how far into the future are we trying to predict , if you modify here, remember to modify in the other files too\n",
        "\n",
        "train_data()\n",
        "NAME = 'LSTM-best.model'\n",
        "model = tf.keras.models.load_model('models/LSTM-best.model')\n",
        "\n",
        "iq = login()\n",
        "\n",
        "i = 0\n",
        "bid = True\n",
        "bets = []\n",
        "lucro = 0\n",
        "MONEY = 100\n",
        "trade = True\n",
        "\n",
        "\n",
        "while (1):\n",
        "    if i >= 10 and i % 2 == 0:\n",
        "        NAME = train_data()+'.model'\n",
        "        model = tf.keras.models.load_model('models/LSTM-best.model')\n",
        "        i = 0\n",
        "    if datetime.datetime.now().second > 30 and i % 2 == 0:  # GARANTE QUE ELE VAI APOSTAR NA SEGUNDA, POIS AQUI ELE JÁ PEGA OS DADOS DE UMA NA FRENTE,\n",
        "        time_taker = time.time()\n",
        "        print(\"Verificando cores..\", end=\"\")\n",
        "        pred_ready = preprocess_prediciton(\n",
        "            iq)  # LOGO, ELE PRECISA DE TEMPO PRA ELABORAR A PREVISÃO ANTES DE ATINGIR OS 59 SEGUNDOS PRA ELE\n",
        "        pred_ready = pred_ready.reshape(1, SEQ_LEN, pred_ready.shape[\n",
        "            3])  # FAZER A APOSTA, ENÃO ELE VAI TENTAR PREVER O VALOR DA TERCEIRA NA FRENTE\n",
        "        result = model.predict(pred_ready)\n",
        "        print('probability of CALL: ', result[0][0])\n",
        "        print('probability of PUT: ', result[0][1])\n",
        "        print(f'Time taken : {int(time.time() - time_taker)} seconds')"
      ],
      "metadata": {
        "id": "N2FtihNwRPdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "235fc855-4102-4d55-c337-faf6f1aae2f4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite a moeda que deseja operar:: EURGBP-OTC\n",
            "STOPLOSS:: 123213\n",
            "Trying to connect to IqOption\n",
            "Successfully Connected!\n",
            "train data: 818 validation: 82\n",
            "sells: 409, buys: 409\n",
            "VALIDATION sells: 41, buys : 41\n",
            "0.007-5-SEQ-2-14-32-PRED-1669516201\n",
            "Epoch 1/14\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.9131 - accuracy: 0.5073\n",
            "Epoch 1: accuracy improved from -inf to 0.50733, saving model to models/LSTM-best.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 31s 923ms/step - loss: 0.9131 - accuracy: 0.5073 - val_loss: 0.6881 - val_accuracy: 0.5000\n",
            "Epoch 2/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.7698 - accuracy: 0.5250\n",
            "Epoch 2: accuracy improved from 0.50733 to 0.52689, saving model to models/LSTM-best.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 23s 916ms/step - loss: 0.7680 - accuracy: 0.5269 - val_loss: 0.6766 - val_accuracy: 0.6098\n",
            "Epoch 3/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.7263 - accuracy: 0.5188\n",
            "Epoch 3: accuracy did not improve from 0.52689\n",
            "26/26 [==============================] - 1s 41ms/step - loss: 0.7268 - accuracy: 0.5196 - val_loss: 0.7052 - val_accuracy: 0.4512\n",
            "Epoch 4/14\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.5073\n",
            "Epoch 4: accuracy did not improve from 0.52689\n",
            "26/26 [==============================] - 1s 42ms/step - loss: 0.7005 - accuracy: 0.5073 - val_loss: 0.6801 - val_accuracy: 0.6220\n",
            "Epoch 5/14\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.5134\n",
            "Epoch 5: accuracy did not improve from 0.52689\n",
            "26/26 [==============================] - 1s 39ms/step - loss: 0.7101 - accuracy: 0.5134 - val_loss: 0.6957 - val_accuracy: 0.4390\n",
            "Epoch 6/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.7052 - accuracy: 0.4925\n",
            "Epoch 6: accuracy did not improve from 0.52689\n",
            "26/26 [==============================] - 1s 40ms/step - loss: 0.7047 - accuracy: 0.4939 - val_loss: 0.6762 - val_accuracy: 0.6341\n",
            "Epoch 7/14\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5086\n",
            "Epoch 7: accuracy did not improve from 0.52689\n",
            "26/26 [==============================] - 1s 40ms/step - loss: 0.6935 - accuracy: 0.5086 - val_loss: 0.6991 - val_accuracy: 0.4634\n",
            "Epoch 8/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.6976 - accuracy: 0.5312\n",
            "Epoch 8: accuracy improved from 0.52689 to 0.52934, saving model to models/LSTM-best.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 20s 815ms/step - loss: 0.6992 - accuracy: 0.5293 - val_loss: 0.6764 - val_accuracy: 0.5732\n",
            "Epoch 9/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.6984 - accuracy: 0.5000\n",
            "Epoch 9: accuracy did not improve from 0.52934\n",
            "26/26 [==============================] - 1s 41ms/step - loss: 0.6986 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5366\n",
            "Epoch 10/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.6958 - accuracy: 0.5312\n",
            "Epoch 10: accuracy did not improve from 0.52934\n",
            "26/26 [==============================] - 2s 65ms/step - loss: 0.6963 - accuracy: 0.5281 - val_loss: 0.6784 - val_accuracy: 0.6341\n",
            "Epoch 11/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.7017 - accuracy: 0.5200\n",
            "Epoch 11: accuracy did not improve from 0.52934\n",
            "26/26 [==============================] - 2s 72ms/step - loss: 0.7020 - accuracy: 0.5183 - val_loss: 0.6964 - val_accuracy: 0.5122\n",
            "Epoch 12/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.6994 - accuracy: 0.5188\n",
            "Epoch 12: accuracy did not improve from 0.52934\n",
            "26/26 [==============================] - 2s 74ms/step - loss: 0.6988 - accuracy: 0.5196 - val_loss: 0.6961 - val_accuracy: 0.4756\n",
            "Epoch 13/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.6984 - accuracy: 0.4913\n",
            "Epoch 13: accuracy did not improve from 0.52934\n",
            "26/26 [==============================] - 1s 41ms/step - loss: 0.6989 - accuracy: 0.4902 - val_loss: 0.6867 - val_accuracy: 0.5732\n",
            "Epoch 14/14\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.6934 - accuracy: 0.5038\n",
            "Epoch 14: accuracy did not improve from 0.52934\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.6934 - accuracy: 0.5024 - val_loss: 0.6886 - val_accuracy: 0.6585\n",
            "Trying to connect to IqOption\n",
            "Successfully Connected!\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "probability of CALL:  0.4180628\n",
            "probability of PUT:  0.5819372\n",
            "Time taken : 4 seconds\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "probability of CALL:  0.4180628\n",
            "probability of PUT:  0.5819372\n",
            "Time taken : 3 seconds\n",
            "Verificando cores.."
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-70dc3400af13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Verificando cores..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         pred_ready = preprocess_prediciton(\n\u001b[0;32m--> 535\u001b[0;31m             iq)  # LOGO, ELE PRECISA DE TEMPO PRA ELABORAR A PREVISÃO ANTES DE ATINGIR OS 59 SEGUNDOS PRA ELE\n\u001b[0m\u001b[1;32m    536\u001b[0m         pred_ready = pred_ready.reshape(1, SEQ_LEN, pred_ready.shape[\n\u001b[1;32m    537\u001b[0m             3])  # FAZER A APOSTA, ENÃO ELE VAI TENTAR PREVER O VALOR DA TERCEIRA NA FRENTE\n",
            "\u001b[0;32m<ipython-input-32-70dc3400af13>\u001b[0m in \u001b[0;36mpreprocess_prediciton\u001b[0;34m(iq)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mactive\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mActives\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactive\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mf'{pariedade}'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-70dc3400af13>\u001b[0m in \u001b[0;36mfast_data\u001b[0;34m(iq, ratio)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0museful_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0museful_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0museful_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museful_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museful_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4911\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4912\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4913\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4914\u001b[0m         )\n\u001b[1;32m   4915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4146\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4148\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4150\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
